{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b6571b22-6ad8-40be-8a6e-3a67cb7a6d42",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "CATALOG_NAME = dbutils.widgets.get(\"catalog\")\n",
    "SCHEMA_NAME = dbutils.widgets.get(\"schema\")\n",
    "table = dbutils.widgets.get(\"table\")\n",
    "\n",
    "path = f\"{CATALOG_NAME}.{SCHEMA_NAME}.{table}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b5cc0890-7957-4708-ad4d-e15cc35c6383",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{\"area\":90,\"conjunto_de_datos\":158,\"url\":1021},\"columnVisibility\":{}},\"settings\":{\"columns\":{\"url\":{\"format\":{\"preset\":\"string-preset-url\"}}}},\"syncTimestamp\":1758681407541}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df = spark.read.table(path)\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c7ede4dd-f911-4a43-bd7b-b6598cb9042f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from urllib.parse import urlparse\n",
    "\n",
    "urls = df.select(\"url\").collect()\n",
    "\n",
    "for row in urls:\n",
    "    url = row['url']\n",
    "    \n",
    "    try:\n",
    "        # Extract filename\n",
    "        parsed_url = urlparse(url)\n",
    "        filename = os.path.basename(parsed_url.path)\n",
    "        table_name = filename.replace('.csv', '').replace('-', '_')\n",
    "        table_path = f\"{CATALOG_NAME}.{SCHEMA_NAME}.{table_name}\"\n",
    "        \n",
    "        # Download to DBFS\n",
    "        dbfs_path = f\"/tmp/{filename}\"\n",
    "        \n",
    "        try:\n",
    "            dbutils.fs.cp(url, f\"dbfs:{dbfs_path}\")\n",
    "        except Exception as download_error:\n",
    "            print(f\"Skipping {filename}: File not found or download failed - {str(download_error)}\")\n",
    "            continue\n",
    "        \n",
    "        # Load CSV\n",
    "        try:\n",
    "            df_csv = spark.read.format(\"csv\").option(\"header\", \"true\").option(\"inferSchema\", \"true\").load(f\"dbfs:{dbfs_path}\")\n",
    "        except Exception as read_error:\n",
    "            print(f\"Skipping {filename}: Failed to read CSV - {str(read_error)}\")\n",
    "            # Clean up the downloaded file if it exists\n",
    "            try:\n",
    "                dbutils.fs.rm(f\"dbfs:{dbfs_path}\")\n",
    "            except:\n",
    "                pass\n",
    "            continue\n",
    "        \n",
    "        # Drop table if it exists to avoid schema conflicts\n",
    "        spark.sql(f\"DROP TABLE IF EXISTS {table_path}\")\n",
    "        \n",
    "        # Create new table\n",
    "        df_csv.write.format(\"delta\").mode(\"overwrite\").saveAsTable(table_path)\n",
    "        spark.sql(f\"ALTER TABLE {table_path} SET TAGS ('concurso' = 'bcie')\")\n",
    "        \n",
    "        print(f\"Created table: {table_name}\")\n",
    "        \n",
    "        # Clean up\n",
    "        try:\n",
    "            dbutils.fs.rm(f\"dbfs:{dbfs_path}\")\n",
    "        except Exception as cleanup_error:\n",
    "            print(f\"Warning: Could not clean up temporary file {dbfs_path} - {str(cleanup_error)}\")\n",
    "            \n",
    "    except Exception as general_error:\n",
    "        print(f\"Unexpected error processing {url}: {str(general_error)}\")\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ee2ceeb3-c718-475b-8587-52113d8b2bc6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "tablas_bcie = spark.sql(f\"\"\"\n",
    "SELECT\n",
    "  tt.catalog_name,\n",
    "  tt.schema_name,\n",
    "  tt.table_name,\n",
    "  tt.tag_name,\n",
    "  tt.tag_value\n",
    "FROM system.information_schema.table_tags tt\n",
    "WHERE tt.tag_name = 'concurso'\n",
    "  AND tt.tag_value = 'bcie'\n",
    "  -- Optional scoping:\n",
    "  AND tt.catalog_name = '{CATALOG_NAME}'\n",
    "  AND tt.schema_name = '{SCHEMA_NAME}'\n",
    "ORDER BY tt.catalog_name, tt.schema_name, tt.table_name;\n",
    "\"\"\")\n",
    "display(tablas_bcie)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5caf6baf-19de-433a-aa19-de4dfd16ca9c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, trim, lit\n",
    "from pyspark.sql import SparkSession\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "import traceback\n",
    "\n",
    "# Use the driver's SparkSession\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "\n",
    "def get_table_description(table_name, catalog_name, schema_name):\n",
    "    \"\"\"\n",
    "    Get table description from DESCRIBE DETAIL command.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        detail_result = spark.sql(f\"DESCRIBE DETAIL {catalog_name}.{schema_name}.{table_name}\")\n",
    "        # Get the description from the result - it's typically in the 'description' column\n",
    "        description_row = detail_result.select(\"description\").collect()\n",
    "        if description_row and description_row[0][\"description\"]:\n",
    "            return description_row[0][\"description\"]\n",
    "        else:\n",
    "            return None\n",
    "    except Exception as e:\n",
    "        print(f\"Warning: Could not get description for {table_name}: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "def process_single_table(table_name, catalog_name, schema_name):\n",
    "    \"\"\"\n",
    "    Process a single table to extract metadata and description.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Get table description first\n",
    "        table_description = get_table_description(table_name, catalog_name, schema_name)\n",
    "        \n",
    "        # Get column metadata using DESCRIBE TABLE EXTENDED\n",
    "        metadata = spark.sql(f\"DESCRIBE TABLE EXTENDED {catalog_name}.{schema_name}.{table_name}\")\n",
    "        \n",
    "        # Filter and transform metadata\n",
    "        table_metadata = metadata.filter(\n",
    "            col(\"col_name\").isNotNull() &\n",
    "            (trim(col(\"col_name\")) != \"\") &\n",
    "            col(\"comment\").isNotNull() &\n",
    "            (trim(col(\"comment\")) != \"\")\n",
    "        ).select(\n",
    "            lit(catalog_name).alias(\"catalog\"),\n",
    "            lit(schema_name).alias(\"schema\"),\n",
    "            lit(table_name).alias(\"table_name\"),\n",
    "            lit(table_description).alias(\"table_description\"),  # Add table description\n",
    "            col(\"col_name\").alias(\"column_name\"),\n",
    "            col(\"data_type\").alias(\"data_type\"),\n",
    "            col(\"comment\").alias(\"comment\")\n",
    "        )\n",
    "        \n",
    "        # Collect rows to driver\n",
    "        rows = table_metadata.collect()\n",
    "        return {\"table\": table_name, \"rows\": rows}\n",
    "        \n",
    "    except Exception as e:\n",
    "        return {\"table\": table_name, \"error\": str(e), \"trace\": traceback.format_exc()}\n",
    "\n",
    "def process_tables_parallel(table_list, catalog_name, schema_name, max_workers=6):\n",
    "    all_rows = []\n",
    "    errors = []\n",
    "    \n",
    "    with ThreadPoolExecutor(max_workers=max_workers) as exec:\n",
    "        futures = {\n",
    "            exec.submit(process_single_table, tbl, catalog_name, schema_name): tbl\n",
    "            for tbl in table_list\n",
    "        }\n",
    "        for fut in as_completed(futures):\n",
    "            res = fut.result()\n",
    "            if \"rows\" in res:\n",
    "                all_rows.extend(res[\"rows\"])\n",
    "                print(f\"✓ {res['table']}: {len(res['rows'])} columns\")\n",
    "            else:\n",
    "                errors.append(res)\n",
    "                print(f\"✗ {res['table']}: {res['error']}\")\n",
    "    \n",
    "    return all_rows, errors\n",
    "\n",
    "# Main execution\n",
    "table_list = [r[\"table_name\"] for r in tablas_bcie.select(\"table_name\").collect()]\n",
    "rows, errors = process_tables_parallel(table_list, CATALOG_NAME, SCHEMA_NAME)\n",
    "\n",
    "# Build DataFrame with updated schema to include table description\n",
    "from pyspark.sql.types import StructType, StructField, StringType\n",
    "schema = StructType([\n",
    "    StructField(\"catalog\", StringType()),\n",
    "    StructField(\"schema\", StringType()),\n",
    "    StructField(\"table_name\", StringType()),\n",
    "    StructField(\"table_description\", StringType()),  # New field for table description\n",
    "    StructField(\"column_name\", StringType()),\n",
    "    StructField(\"data_type\", StringType()),\n",
    "    StructField(\"comment\", StringType())\n",
    "])\n",
    "\n",
    "final_df = spark.createDataFrame(rows, schema)\n",
    "final_df.write.mode(\"overwrite\").saveAsTable(f\"{CATALOG_NAME}.{SCHEMA_NAME}.table_metadata\")\n",
    "display(final_df)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "dbe_65bc13ea-276c-4905-a728-9fe2fb1780e2",
    "environment_version": "3"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 2
   },
   "notebookName": "Carga de Datasets",
   "widgets": {
    "catalog": {
     "currentValue": "users",
     "nuid": "d5453dde-c718-4c22-acba-e0cecbbe4077",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "users",
      "label": "",
      "name": "catalog",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "users",
      "label": "",
      "name": "catalog",
      "options": {
       "widgetType": "text",
       "autoCreated": false,
       "validationRegex": null
      }
     }
    },
    "schema": {
     "currentValue": "kevin_romero",
     "nuid": "58b8f0ec-9b37-48a4-a35a-db68b50e778c",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "kevin_romero",
      "label": "",
      "name": "schema",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "kevin_romero",
      "label": "",
      "name": "schema",
      "options": {
       "widgetType": "text",
       "autoCreated": false,
       "validationRegex": null
      }
     }
    },
    "table": {
     "currentValue": "bcie_datasets",
     "nuid": "eb02915f-f4d5-41ab-8d4a-191310c32109",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "bcie_datasets",
      "label": "",
      "name": "table",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "bcie_datasets",
      "label": "",
      "name": "table",
      "options": {
       "widgetType": "text",
       "autoCreated": false,
       "validationRegex": null
      }
     }
    }
   }
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
